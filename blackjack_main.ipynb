{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uT2EU5jNhHuk"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from gym import spaces\n",
        "import matplotlib.colors as mcolors\n",
        "import seaborn as sns\n",
        "import time\n",
        "from itertools import product"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1asOgQ5tn8Ky"
      },
      "source": [
        "###MDP solution approaches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jiil3k4EfVS"
      },
      "outputs": [],
      "source": [
        "class Sol_Env(gym.Env):\n",
        "    def __init__(self):\n",
        "        # Define ranges for each state variable\n",
        "        self.player_sum_1_range = range(4, 31)  # Current sum 1\n",
        "        self.dealer_sum_range = range(4, 29)  # Example range for dealer sum\n",
        "        self.usable_ace_1 = [False, True]  # Usable ace for hand 1\n",
        "        self.stick_happened = [False, True]  # end or not\n",
        "\n",
        "        # Define actions\n",
        "        self.actions = ['hit', 'stick']\n",
        "\n",
        "    def get_all_states(self):\n",
        "        \"\"\"Generate all possible states.\"\"\"\n",
        "        states = list(product(\n",
        "            self.player_sum_1_range,  # Current sum 1\n",
        "            self.dealer_sum_range,  # Dealer's showing card\n",
        "            self.usable_ace_1,  # Usable ace for player\n",
        "            #self.usable_ace_2,  # Usable ace for dealer\n",
        "            self.stick_happened  # end or not\n",
        "        ))\n",
        "        filtered_states = [\n",
        "            state for state in states\n",
        "\n",
        "            # Filter out states where player_sum < 12 and usable_ace = True\n",
        "            if not (state[0] < 12 and state[2])\n",
        "            and not (state[0] >= 21 and not state[3])\n",
        "            and not (state[1] >= 21 and not state[3])\n",
        "                           ]\n",
        "        return filtered_states\n",
        "\n",
        "    def get_possible_actions(self, state):\n",
        "        \"\"\"Return possible actions for a given state.\"\"\"\n",
        "        player_sum, dealer_sum, _, stick_happened = state\n",
        "        if player_sum >= 21 or dealer_sum >= 21 or (dealer_sum >= 17 and stick_happened):\n",
        "            return []\n",
        "        elif stick_happened:\n",
        "            return ['stick']\n",
        "        elif dealer_sum >= 17 and not stick_happened:\n",
        "            return ['hit']\n",
        "        actions = ['hit', 'stick']\n",
        "        return actions\n",
        "\n",
        "    def get_reward(self, player_sum_1, dealer_sum, stick_happened):\n",
        "        \"\"\"\n",
        "        Calculate the reward for a given state.\n",
        "        \"\"\"\n",
        "\n",
        "        if player_sum_1 >= 21 or dealer_sum >= 21 or (dealer_sum >= 17 and stick_happened):\n",
        "          if player_sum_1 > 21 and dealer_sum == 21 or dealer_sum == 21 and player_sum_1 < 21 or dealer_sum < 21 and player_sum_1 > 21:\n",
        "                return -1\n",
        "          elif player_sum_1 == 21 and dealer_sum > 21 or dealer_sum < 21 and player_sum_1 == 21 or dealer_sum > 21 and player_sum_1 < 21:\n",
        "                return 1\n",
        "\n",
        "          elif player_sum_1 == 21 and dealer_sum == 21 or dealer_sum > 21 and player_sum_1 > 21:\n",
        "                return 0\n",
        "\n",
        "          else:\n",
        "            diff_21_player = 21 - player_sum_1\n",
        "            diff_21_dealer = 21 - dealer_sum\n",
        "            if diff_21_player > diff_21_dealer:\n",
        "                return -1\n",
        "            elif diff_21_player < diff_21_dealer:\n",
        "                return 1\n",
        "            else:\n",
        "                return 0\n",
        "\n",
        "        return 0\n",
        "\n",
        "\n",
        "    def get_transition_probabilities(self, state, action):\n",
        "        \"\"\"\n",
        "        Calculate transition probabilities for a given state-action pair.\n",
        "        \"\"\"\n",
        "        distr = [1 / 13] * 8 + [4 / 13] + [1 / 13]  # Probabilities for cards 2–11\n",
        "        card_values = list(range(2, 12))  # Cards are valued from 2 to 11\n",
        "        distr_dict = {card_values[i]: distr[i] for i in range(len(card_values))}\n",
        "\n",
        "        transitions = []\n",
        "        player_sum_1, dealer_sum, ace_1, stick_happened = state\n",
        "\n",
        "        if action == 'hit':\n",
        "            if player_sum_1 >= 21:\n",
        "                reward = self.get_reward(player_sum_1, dealer_sum, True)\n",
        "                transitions.append((state, 1.0, reward))\n",
        "            else:\n",
        "                for card, prob in distr_dict.items():\n",
        "                    new_sum = player_sum_1 + card\n",
        "                    new_ace_1 = ace_1\n",
        "                    if new_sum > 21 and new_ace_1:\n",
        "                        if card == 11:\n",
        "                          new_sum -= 10\n",
        "                        else:\n",
        "                          new_sum -= 10\n",
        "                          new_ace_1 = False\n",
        "                    if card == 11 and player_sum_1 < 11 and not new_ace_1:\n",
        "                        new_ace_1 = True\n",
        "                    if card == 11 and new_sum > 21 and not new_ace_1:\n",
        "                        new_sum -= 10\n",
        "                    if new_sum >= 21:\n",
        "                        next_state = (new_sum, dealer_sum, new_ace_1, True)\n",
        "                        reward = self.get_reward(new_sum, dealer_sum, True)\n",
        "                        transitions.append((next_state, prob, reward))\n",
        "                    else:\n",
        "                        next_state = (new_sum, dealer_sum, new_ace_1, stick_happened)\n",
        "                        reward = self.get_reward(new_sum, dealer_sum, stick_happened)\n",
        "                        transitions.append((next_state, prob, reward))\n",
        "\n",
        "        elif action == 'stick': # usable ace for the dealer?\n",
        "            if dealer_sum >= 21:\n",
        "                reward = self.get_reward(player_sum_1, dealer_sum, True)\n",
        "                transitions.append((state, 1.0, reward))\n",
        "            else:\n",
        "                if not stick_happened:\n",
        "                  if dealer_sum < 17:\n",
        "                    for card, prob in distr_dict.items():\n",
        "                        new_dealer_sum = dealer_sum + card\n",
        "                        next_state = (player_sum_1, new_dealer_sum, ace_1, True)\n",
        "                        reward = self.get_reward(player_sum_1, new_dealer_sum, True)\n",
        "                        transitions.append((next_state, prob, reward))\n",
        "                  else:\n",
        "                    reward = self.get_reward(player_sum_1, dealer_sum, True)\n",
        "                    transitions.append((state, 1.0, reward))\n",
        "                else:\n",
        "                  if dealer_sum < 17:\n",
        "                    for card, prob in distr_dict.items():\n",
        "                        new_dealer_sum = dealer_sum + card\n",
        "                        next_state = (player_sum_1, new_dealer_sum, ace_1, stick_happened)\n",
        "                        reward = self.get_reward(player_sum_1, new_dealer_sum, stick_happened)\n",
        "                        transitions.append((next_state, prob, reward))\n",
        "                  else:\n",
        "                    reward = self.get_reward(player_sum_1, dealer_sum, stick_happened)\n",
        "                    transitions.append((state, 1.0, reward))\n",
        "\n",
        "        return transitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXUzyYBldKDn"
      },
      "outputs": [],
      "source": [
        "bj = Sol_Env()\n",
        "#len(bj.get_all_states())\n",
        "#bj.get_all_states()[:500]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "br_0jdYXk8Uy"
      },
      "outputs": [],
      "source": [
        "all_states = bj.get_all_states()\n",
        "\n",
        "for state in all_states[500:1000]:\n",
        "    possible_actions = bj.get_possible_actions(state)\n",
        "    for action in possible_actions:\n",
        "        transitions = bj.get_transition_probabilities(state, action)\n",
        "        for next_state, prob, reward in transitions:  # Assuming it's a list of (next_state, probability)\n",
        "            print(f\"State: {state}, Action: {action}, Next State: {next_state}, Probability: {prob}, Reward: {reward}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3aIN47E1iGF"
      },
      "outputs": [],
      "source": [
        "print(f\"Total states: {len(bj.get_all_states())}\")\n",
        "terminal_states = [s for s in bj.get_all_states() if not bj.get_possible_actions(s)]\n",
        "print(f\"Number of terminal states: {len(terminal_states)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yahDOUn__G3V"
      },
      "outputs": [],
      "source": [
        "for i in terminal_states:\n",
        "  print(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRiyg6u_02b1"
      },
      "source": [
        "Value iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEVki-Z-ENdb"
      },
      "outputs": [],
      "source": [
        "def value_iteration_to_get_opt_solution(env, n=1000, theta=1e-15):\n",
        "    \"\"\"Compute the optimal value function V* using value iteration.\"\"\"\n",
        "    all_states = env.get_all_states()\n",
        "    non_terminal_states = [s for s in all_states if not (s[0] >= 21 or s[1] >= 21 or (s[1] >= 17 and s[3]))]\n",
        "\n",
        "    V_opt = {state: 0 for state in all_states}  # Initialize V*\n",
        "    policy = {state: None for state in all_states}  # Store optimal policy\n",
        "\n",
        "    for _ in range(n):\n",
        "        delta = 0\n",
        "        W = V_opt.copy()\n",
        "\n",
        "        for state in non_terminal_states:\n",
        "            old_value = V_opt[state]\n",
        "            max_value = float('-inf')\n",
        "            best_action = None\n",
        "\n",
        "            for action in env.get_possible_actions(state):\n",
        "                transitions = env.get_transition_probabilities(state, action)\n",
        "                action_value = sum(prob * (reward + W[next_state]) for next_state, prob, reward in transitions)\n",
        "\n",
        "                if action_value > max_value:\n",
        "                    max_value = action_value\n",
        "                    best_action = action\n",
        "\n",
        "            V_opt[state] = max_value\n",
        "            policy[state] = best_action\n",
        "            delta = max(delta, abs(old_value - max_value))\n",
        "\n",
        "        if delta < theta:\n",
        "            break\n",
        "\n",
        "    return V_opt, policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06QaX9WpcqWK"
      },
      "outputs": [],
      "source": [
        "V_opt, pol_opt = value_iteration_to_get_opt_solution(bj)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62NP8N8WfqKq"
      },
      "outputs": [],
      "source": [
        "pol_opt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UefqQA-Vd0vT"
      },
      "outputs": [],
      "source": [
        "zero_value_states = [state for state, value in V_opt.items() if value == 0]\n",
        "for i in zero_value_states:\n",
        "  print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oc10lnTBIrn7"
      },
      "outputs": [],
      "source": [
        "print(terminal_states == zero_value_states)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcv0bKNmwluO"
      },
      "outputs": [],
      "source": [
        "def value_iteration(env, theta=1e-15, gamma=1, n=1000):\n",
        "    \"\"\"Perform value iteration and track sup-norm error.\"\"\"\n",
        "    all_states = env.get_all_states()\n",
        "    non_terminal_states = [s for s in all_states if not (s[0] >= 21 or s[1] >= 21 or (s[1] >= 17 and s[3]))]\n",
        "\n",
        "    V = {state: 0 for state in all_states}  # Initialize V\n",
        "    policy = {state: None for state in all_states}  # Initialize policy\n",
        "    sup_norm_errors = []  # Track sup norm differences\n",
        "    iterations = 0\n",
        "\n",
        "    for _ in range(n):\n",
        "        delta = 0\n",
        "        W = V.copy()\n",
        "\n",
        "        for state in non_terminal_states:\n",
        "            old_value = V[state]\n",
        "            max_value = float('-inf')\n",
        "            best_action = None\n",
        "\n",
        "            for action in env.get_possible_actions(state):\n",
        "                transitions = env.get_transition_probabilities(state, action)\n",
        "                action_value = sum(prob * (reward + W[next_state]) for next_state, prob, reward in transitions)\n",
        "\n",
        "                if action_value > max_value:\n",
        "                    max_value = action_value\n",
        "                    best_action = action\n",
        "\n",
        "            V[state] = max_value\n",
        "            policy[state] = best_action\n",
        "            delta = max(delta, abs(old_value - max_value))\n",
        "\n",
        "        # Compute sup norm error\n",
        "        sup_norm = max(abs(V[state] - V_opt[state]) for state in non_terminal_states)\n",
        "        sup_norm_errors.append(sup_norm)\n",
        "        iterations += 1\n",
        "        if delta < theta:\n",
        "            break\n",
        "\n",
        "    return V, policy, iterations, sup_norm_errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zt-FzhLV04rD"
      },
      "outputs": [],
      "source": [
        "bj = Sol_Env()\n",
        "start_time = time.time()\n",
        "#V_vi, policy_vi, iterations_vi, deltas_vi = value_iteration(bj, V_opt)\n",
        "V_vi, policy_vi, iterations_vi, deltas_vi = value_iteration(bj)\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\"Value Iteration completed in {end_time - start_time:.2f} seconds and {iterations_vi} iterations.\")\n",
        "# Plot convergence\n",
        "def plot_convergence(deltas, label, name, step=1):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(len(deltas)), deltas)\n",
        "    #plt.plot(range(1, len(deltas)+1), deltas)\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Delta')\n",
        "    plt.title(label)\n",
        "    plt.xticks(range(0, len(deltas), step))\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.savefig(name, format=\"jpg\", dpi=300)\n",
        "    plt.show()\n",
        "plot_convergence(deltas_vi, 'Convergence of Value Iteration', \"value_iteration.jpg\", 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRGFwfTfBkc1"
      },
      "outputs": [],
      "source": [
        "print(V_opt == V_vi)\n",
        "print(pol_opt == policy_vi)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8UuCaqhzpec"
      },
      "source": [
        "Gauss-Seidel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sn9sNRP6n8T5"
      },
      "outputs": [],
      "source": [
        "def gauss_seidel(env, V_opt, n = 100, theta=1e-15):\n",
        "    \"\"\"Perform value iteration to find the optimal policy.\"\"\"\n",
        "    # Get all states and filter out terminal ones\n",
        "    all_states = env.get_all_states()\n",
        "    non_terminal_states = [state for state in all_states if not(state[0] >= 21 or state[1] >= 21 or (state[1] >= 17 and state[3]))]\n",
        "    #non_terminal_states = [state for state in all_states if not(state[0] >= 21 or state[1] >= 21 or state[1] > 17 and state[4])]\n",
        "\n",
        "    V = {state: 0 for state in all_states}  # Initialize value function\n",
        "    policy = {state: None for state in all_states}  # Initialize policy\n",
        "\n",
        "    iterations = 0\n",
        "    delta_list = []  # To track convergence\n",
        "\n",
        "    for i in range(n):\n",
        "    #while iterations != n:\n",
        "        for state in non_terminal_states:\n",
        "            old_value = V[state]\n",
        "            max_value = float('-inf')\n",
        "            best_action = None\n",
        "            #print(state)\n",
        "            for action in env.get_possible_actions(state):\n",
        "                #print(action)\n",
        "                transitions = env.get_transition_probabilities(state, action)\n",
        "                #for i in transitions:\n",
        "                  #print(i)\n",
        "                action_value = sum(\n",
        "                    prob * (reward + V[next_state])\n",
        "                    for next_state, prob, reward in transitions\n",
        "                )\n",
        "                if action_value > max_value:\n",
        "                    max_value = action_value\n",
        "                    best_action = action\n",
        "\n",
        "            V[state] = max_value\n",
        "            policy[state] = best_action\n",
        "\n",
        "        diff = max(abs(V_opt[key] - V[key]) for key in V)\n",
        "        delta_list.append(diff)\n",
        "        iterations += 1\n",
        "        #if diff == 0:\n",
        "        if diff < theta:\n",
        "            break\n",
        "\n",
        "    return V, policy, iterations, delta_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ffOzzYaSiVh"
      },
      "outputs": [],
      "source": [
        "# Main Execution\n",
        "bj = Sol_Env()\n",
        "\n",
        "start_time = time.time()\n",
        "V_gs, policy_gs, iterations_gs, deltas_gs = gauss_seidel(bj, V_opt)\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\"Gauss-Seidel completed in {end_time - start_time:.2f} seconds and {iterations_gs} iterations.\")\n",
        "#print(deltas)\n",
        "plot_convergence(deltas_gs, 'Convergence of Gauss-Seidel', \"gauss-seidel.jpg\", 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__aTVDxUB4ts"
      },
      "outputs": [],
      "source": [
        "print(V_opt == V_gs)\n",
        "print(pol_opt == policy_gs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jq5P2dnGoBvB"
      },
      "source": [
        "Optimistic Policy iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwNuW9Jow1DU"
      },
      "outputs": [],
      "source": [
        "def opt_policy_iteration(env, n=100, theta=1e-15):\n",
        "    \"\"\"Perform policy iteration and track sup-norm error.\"\"\"\n",
        "    all_states = env.get_all_states()\n",
        "    non_terminal_states = [s for s in all_states if not (s[0] >= 21 or s[1] >= 21 or (s[1] > 17 and s[3]))]\n",
        "\n",
        "    V = {state: 0 for state in all_states}  # Initialize V\n",
        "    # Initialize policy and value function\n",
        "    policy = {}\n",
        "    for state in all_states:\n",
        "        possible_actions = env.get_possible_actions(state)\n",
        "        if possible_actions:  # Ensure there are valid actions\n",
        "            policy[state] = possible_actions[0]  # Default to the first action\n",
        "        else:\n",
        "            policy[state] = None  # No action for terminal states\n",
        "    sup_norm_errors = []  # Track sup norm differences\n",
        "    iterations = 0\n",
        "    iterations2 = 0\n",
        "    for _ in range(n):\n",
        "        # Policy Evaluation\n",
        "        while True:\n",
        "            delta = 0\n",
        "            for state in non_terminal_states:\n",
        "                old_value = V[state]\n",
        "                action = policy[state]\n",
        "                if action is None:\n",
        "                    continue\n",
        "                transitions = env.get_transition_probabilities(state, action)\n",
        "                if not transitions:\n",
        "                    continue\n",
        "                V[state] = sum(prob * (reward + V[next_state]) for next_state, prob, reward in transitions)\n",
        "                delta = max(delta, abs(old_value - V[state]))\n",
        "            iterations2 += 1\n",
        "            if delta < theta:\n",
        "                break\n",
        "\n",
        "        # Policy Improvement\n",
        "        policy_stable = True\n",
        "        for state in non_terminal_states:\n",
        "            old_action = policy[state]\n",
        "            best_action = None\n",
        "            max_value = float('-inf')\n",
        "\n",
        "            for action in env.get_possible_actions(state):\n",
        "                transitions = env.get_transition_probabilities(state, action)\n",
        "                action_value = sum(prob * (reward + V[next_state]) for next_state, prob, reward in transitions)\n",
        "\n",
        "                if action_value > max_value:\n",
        "                    max_value = action_value\n",
        "                    best_action = action\n",
        "\n",
        "            if best_action is not None:\n",
        "                policy[state] = best_action\n",
        "                if old_action != best_action:\n",
        "                    policy_stable = False\n",
        "\n",
        "        # Compute sup norm error\n",
        "        sup_norm = max(abs(V[state] - V_opt[state]) for state in non_terminal_states)\n",
        "        sup_norm_errors.append(sup_norm)\n",
        "        iterations += 1\n",
        "        if policy_stable:\n",
        "            break\n",
        "\n",
        "    return V, policy, iterations, iterations2, sup_norm_errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwAmOypwDfba"
      },
      "outputs": [],
      "source": [
        "bj = Sol_Env()\n",
        "\n",
        "start_time = time.time()\n",
        "#V_pi, policy_pi, iterations_pi, deltas_pi, deltas_pi2 = opt_policy_iteration(bj, V_opt)\n",
        "V_pi, policy_pi, iterations_pi, iterations_pi2, deltas_pi = opt_policy_iteration(bj)\n",
        "end_time = time.time()\n",
        "\n",
        "\n",
        "# Trim the list until the first 0 element\n",
        "trimmed_deltas = deltas_pi[:next((i for i, x in enumerate(deltas_pi) if x == 0), len(deltas_pi))]\n",
        "print(f\"Optimistic Policy Iteration completed in {end_time - start_time:.2f} seconds and {len(trimmed_deltas)} iterations.\")\n",
        "plot_convergence(deltas_pi, 'Convergence of Policy Iteration', 's.jpg')\n",
        "#plot_convergence(trimmed_deltas, 'Convergence of Policy Iteration', 's.jpg')\n",
        "print(iterations_pi2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHIAyXiYAG2b"
      },
      "outputs": [],
      "source": [
        "print(deltas_pi)\n",
        "#print(policy_pi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FnVK90hgSZi8"
      },
      "outputs": [],
      "source": [
        "print(V_opt == V_pi)\n",
        "print(pol_opt == policy_pi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GovBOAqD04Z"
      },
      "outputs": [],
      "source": [
        "print(deltas_pi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zAFq85IDHgh"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(deltas_vi, label=\"Value Iteration\", linewidth=2)\n",
        "plt.plot(deltas_gs, label=\"Gauss-Seidel Value Iteration\", linestyle=\"--\", linewidth=2)\n",
        "plt.plot(deltas_pi, label=\"Policy Iteration\", linewidth=2)\n",
        "\n",
        "plt.xlabel(\"Number of Iterations\")\n",
        "plt.ylabel(\"Sup Norm Difference\")\n",
        "plt.title(\"Convergence of MDP Methods\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig('MDP_conv.jpg', format=\"jpg\", dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeUwsJBbAitd"
      },
      "source": [
        "###RL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AClrB1SlhHuk"
      },
      "outputs": [],
      "source": [
        "class BlackJackEnv(gym.Env):\n",
        "\n",
        "    metadata = {'render.modes':['human']}\n",
        "\n",
        "    def __init__(self):\n",
        "        self.observation_space = spaces.Discrete(2688)\n",
        "        self.action_space = spaces.Discrete(2)\n",
        "        self.step_count = 0                        ### Number of actions taken in the game till now\n",
        "        self.actions = ['hit', 'stick']\n",
        "\n",
        "\n",
        "    def check_usable_ace(self,hand):\n",
        "        ### Creating a temporary hand taking the Ace's value as 11 to check of usability\n",
        "        temp_hand = hand.copy()\n",
        "\n",
        "        ### Checking if the hand has any ace, if not then returns False\n",
        "        if np.any(temp_hand == 11) and temp_hand.sum() > 21:\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def use_ace(self,hand):\n",
        "        temp_hand = hand.copy()\n",
        "        temp_hand[np.where(temp_hand == 11)[0][0]] = 1\n",
        "        return temp_hand\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        distr = [1/13] * 9 + [4/13]\n",
        "        ### New Player Hand\n",
        "        self.current_hand = np.random.choice(range(2, 12), 2, p=distr)\n",
        "\n",
        "        ### Initialising Usable Ace as False\n",
        "\n",
        "        self.usable_ace = False\n",
        "\n",
        "        ### Variable is used to inform whether the dealer has sticked,\n",
        "        ### Used to know when to terminate the game\n",
        "\n",
        "        self.stick_happened = False\n",
        "\n",
        "\n",
        "        ### Checking if player hand has Usable Ace, if yes, then replacing it with 11.\n",
        "        if self.check_usable_ace(self.current_hand):\n",
        "            self.usable_ace = True\n",
        "            self.current_hand = self.use_ace(self.current_hand)\n",
        "\n",
        "        ### State variable Current Sum\n",
        "        self.current_sum = self.current_hand.sum()\n",
        "\n",
        "        ### Dealer's New Hand\n",
        "        self.dealer_hand = np.random.choice(range(2, 12), 2, p=distr)\n",
        "\n",
        "        ### Dealer's Sum\n",
        "        self.dealer_sum = self.dealer_hand.sum()\n",
        "\n",
        "        ### State Variable: Dealer Showing Card\n",
        "        self.dealer_showing_card = self.dealer_hand[0]\n",
        "\n",
        "        ### Checking if Dealer's hand has Usable Ace, if yes, then replacing it with 11.\n",
        "        if self.check_usable_ace(self.dealer_hand):\n",
        "            temp_dealer_hand = self.use_ace(self.dealer_hand)\n",
        "            self.dealer_sum = temp_dealer_hand.sum()\n",
        "\n",
        "\n",
        "    def take_turn(self,player):\n",
        "\n",
        "        distr = [1/13] * 9 + [4/13]\n",
        "\n",
        "        if player == 'dealer':\n",
        "\n",
        "            ### takes a new random card\n",
        "            new_card = np.random.choice(range(1, 11), p=distr)\n",
        "\n",
        "            ### adding new card to the players hand and making a temporary new hand\n",
        "            new_dealer_hand = np.array(self.dealer_hand.tolist() +  [new_card])\n",
        "\n",
        "            ### Check if there is usable ace\n",
        "            if self.check_usable_ace(new_dealer_hand):\n",
        "\n",
        "                ### replace ace(1) with 11\n",
        "                new_dealer_hand = self.use_ace(new_dealer_hand)\n",
        "\n",
        "            ### Assigning the temporary hand to the players actual hand\n",
        "            self.dealer_hand = new_dealer_hand\n",
        "\n",
        "            ### Updating the players hand sum variable\n",
        "            self.dealer_sum = self.dealer_hand.sum()\n",
        "\n",
        "        if player == 'player':\n",
        "\n",
        "            ### takes a new random card\n",
        "            new_card = np.random.choice(range(1, 11), p=distr)\n",
        "\n",
        "            ### adding new card to the players hand and making a temporary new hand\n",
        "            new_player_hand = np.array(self.current_hand.tolist()+ [new_card])\n",
        "\n",
        "            ### Check if there is usable ace\n",
        "            if self.check_usable_ace(new_player_hand):\n",
        "\n",
        "                ### replace ace(1) with 11\n",
        "                self.usable_ace = True\n",
        "                new_player_hand = self.use_ace(new_player_hand)\n",
        "\n",
        "            ### Assigning the temporary hand to the players actual hand\n",
        "            self.current_hand = new_player_hand\n",
        "            ### Updating the players hand sum variable\n",
        "            self.current_sum = self.current_hand.sum()\n",
        "\n",
        "\n",
        "\n",
        "    def check_game_status(self, mode = 'normal'):\n",
        "\n",
        "        '''\n",
        "         checks the status of the game, there are two modes\n",
        "         'normal' mode - the default mode, this is used to check after\n",
        "                         each turn whether a terminal state has been reached\n",
        "         'compare' mode - used when we need to compare the totals of both the players\n",
        "                          to judge the winner\n",
        "\n",
        "\n",
        "         returns a result dictionary with the winner, whether the game is finished\n",
        "         and the reward of the game\n",
        "        '''\n",
        "        result = {'winner':'',\n",
        "                 'is_done': False,\n",
        "                 'reward':0}\n",
        "\n",
        "\n",
        "        if self.current_sum > 21:\n",
        "            self.stick_happened = True\n",
        "            result['winner'] = 'dealer'\n",
        "            result['is_done'] = True\n",
        "            result['reward'] = -1\n",
        "        elif self.dealer_sum > 21:\n",
        "            self.stick_happened = True\n",
        "            result['winner'] = 'player'\n",
        "            result['is_done'] = True\n",
        "            result['reward'] = 1\n",
        "\n",
        "        elif self.current_sum == 21:\n",
        "            self.stick_happened = True\n",
        "            result['winner'] = 'player'\n",
        "            result['is_done'] = True\n",
        "            result['reward'] = 1\n",
        "\n",
        "        elif self.dealer_sum == 21:\n",
        "            self.stick_happened = True\n",
        "            result['winner'] = 'dealer'\n",
        "            result['is_done'] = True\n",
        "            result['reward'] = -1\n",
        "\n",
        "        else:\n",
        "            if self.stick_happened and self.dealer_sum > 17:\n",
        "                result['is_done'] = True\n",
        "                diff_21_player = 21 - self.current_sum\n",
        "                diff_21_dealer = 21 - self.dealer_sum\n",
        "\n",
        "                if diff_21_player > diff_21_dealer:\n",
        "                    result['reward'] = -1\n",
        "                    result['winner'] = 'dealer'\n",
        "                elif diff_21_player < diff_21_dealer:\n",
        "                    result['reward'] = 1\n",
        "                    result['winner'] = 'player'\n",
        "                else:\n",
        "                    result['reward'] = 0\n",
        "                    result['winner'] = 'draw'\n",
        "\n",
        "                return result\n",
        "\n",
        "        return result\n",
        "\n",
        "    def step(self,action):\n",
        "\n",
        "        '''\n",
        "        Performs one action, either Hit or Stick\n",
        "\n",
        "        returns - a result dictionary with the winner, whether the game is finished\n",
        "        and the reward of the game\n",
        "\n",
        "        '''\n",
        "\n",
        "        self.step_count += 1  ### Number of actions taken in the game till now\n",
        "\n",
        "\n",
        "        result = {'winner':'',\n",
        "                 'is_done': False,\n",
        "                 'reward':0}\n",
        "\n",
        "        ### Before taking the first step of the game we need to check for \"natural\"\n",
        "        ### winning condition if the initial two cards of the players are 21\n",
        "        ### If anyone has 21, then that player wins, if both have 21, then the game is\n",
        "        ### drawn. Otherwise the game will continue\n",
        "\n",
        "        if self.step_count == 1:\n",
        "            if self.check_usable_ace(self.current_hand):\n",
        "                self.current_hand = self.use_ace(self.current_hand)\n",
        "            if self.check_usable_ace(self.dealer_hand):\n",
        "                self.current_hand = self.use_ace(self.dealer_hand)\n",
        "\n",
        "            if self.current_sum == 21 and self.dealer_sum == 21:\n",
        "                self.stick_happened = True\n",
        "                result['is_done'] = True\n",
        "                result['reward'] = 0\n",
        "                result['winner'] = 'draw'\n",
        "                return result\n",
        "\n",
        "            elif self.current_sum == 21 and self.dealer_sum < 21:\n",
        "                self.stick_happened = True\n",
        "                result['is_done'] = True\n",
        "                result['reward'] = 1\n",
        "                result['winner'] = 'player'\n",
        "                return result\n",
        "\n",
        "            elif self.dealer_sum == 21 and self.current_sum < 21:\n",
        "                self.stick_happened = True\n",
        "                result['is_done'] = True\n",
        "                result['reward'] = -1\n",
        "                result['winner'] = 'dealer'\n",
        "                return result\n",
        "\n",
        "            if self.dealer_sum >= 17:\n",
        "                self.stick_happened = True\n",
        "\n",
        "        ### action = 0, meaning \"hit\"\n",
        "\n",
        "        if action == 0 and not self.stick_happened:\n",
        "\n",
        "            ### Player Takes Turn\n",
        "            self.take_turn('player')\n",
        "\n",
        "            ### Checking game status\n",
        "            result = self.check_game_status()\n",
        "            if result['is_done'] == True:\n",
        "                return result\n",
        "\n",
        "\n",
        "\n",
        "        if action == 1:  ### stick\n",
        "\n",
        "            self.stick_happened = True\n",
        "\n",
        "            ### Dealers Turn\n",
        "            if self.dealer_sum <= 17:\n",
        "\n",
        "                self.take_turn('dealer')\n",
        "                result = self.check_game_status()\n",
        "            else:\n",
        "                result = self.check_game_status()\n",
        "                result['is_done'] == True\n",
        "                return result\n",
        "\n",
        "        \"\"\"if action == 1:  ### stick\n",
        "\n",
        "            self.stick_happened = True\n",
        "\n",
        "            if self.stick_happened == True and self.dealer_sum > 17:\n",
        "                  result = self.check_game_status()\n",
        "                  result['is_done'] == True\n",
        "                  return result\n",
        "\n",
        "            ### Dealers Turn\n",
        "            while self.dealer_sum <= 17:\n",
        "\n",
        "                self.take_turn('dealer')\n",
        "                result = self.check_game_status()\n",
        "                if result['is_done'] == True:\n",
        "                    return result\"\"\"\n",
        "\n",
        "        return result\n",
        "\n",
        "\n",
        "    def get_current_state(self):\n",
        "        '''\n",
        "        returns the current state variables, current_sum, dealer_showing_card, usable_ace\n",
        "        '''\n",
        "        current_state = {}\n",
        "\n",
        "        current_state['current_sum'] = self.current_sum\n",
        "        current_state['dealer_sum'] = self.dealer_sum\n",
        "        current_state['usable_ace'] = self.usable_ace\n",
        "        current_state['stick_happened'] = self.stick_happened\n",
        "\n",
        "        return current_state\n",
        "\n",
        "\n",
        "    def render(self):\n",
        "\n",
        "        print('OBSERVABLE STATES')\n",
        "        print('Current Sum - {}'.format(self.current_sum))\n",
        "        print('Dealer Sum - {}'.format(self.dealer_sum))\n",
        "        print('Usable Ace - {}'.format(self.usable_ace))\n",
        "        print('Stick happened - {}'.format(self.stick_happened))\n",
        "\n",
        "        print('AUXILLARY INFORMATION ------------------------------')\n",
        "        print('Current Hand - {}'.format(self.current_hand))\n",
        "        print('Dealer Hand - {}'.format(self.dealer_hand))\n",
        "        print('Dealer Showing Card - {}'.format(self.dealer_showing_card))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEkr4TVhhHum"
      },
      "outputs": [],
      "source": [
        "bj = BlackJackEnv() #a double down miatt mindig újra kell ezt indítani"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jjqJvXYhHum",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "bj.reset()\n",
        "bj.render()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvFKDXLKhHun"
      },
      "outputs": [],
      "source": [
        "print(bj.step(1))\n",
        "bj.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFLU_MEuhHup"
      },
      "source": [
        "#### Q-Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjGEopHzhHup"
      },
      "outputs": [],
      "source": [
        "#### following are 4 dictionaries which help in converting the\n",
        "#### state values like current_sum and action to indexes in the Q value table\n",
        "\n",
        "current_sum_to_index = dict(zip(np.arange(4,33),np.arange(28)))\n",
        "dealer_sum_to_index = dict(zip(np.arange(4,29),np.arange(24)))\n",
        "usable_ace_index = dict(zip([False,True],[0,1]))\n",
        "stick_happened_index = dict(zip([False,True],[0,1]))\n",
        "action_index = dict(zip(['hit','stick'],[0,1]))\n",
        "\n",
        "def get_state_q_indices(current_state):\n",
        "\n",
        "    '''\n",
        "    used to get indices of the Q table for any given state\n",
        "\n",
        "    '''\n",
        "    current_sum_idx = current_sum_to_index[current_state['current_sum']]\n",
        "    dealer_sum_idx = dealer_sum_to_index[current_state['dealer_sum']]\n",
        "    usable_ace_idx = usable_ace_index[current_state['usable_ace']]\n",
        "    stick_happened_idx = stick_happened_index[current_state['stick_happened']]\n",
        "\n",
        "    return [current_sum_idx, dealer_sum_idx, usable_ace_idx, stick_happened_idx]\n",
        "\n",
        "def get_max_action(Q_sa, current_state):\n",
        "\n",
        "    '''\n",
        "    used to get the action with the max q-value given the current state and the Q table\n",
        "\n",
        "    '''\n",
        "\n",
        "    state_q_idxs = get_state_q_indices(current_state)\n",
        "    action = Q_sa[state_q_idxs[0],state_q_idxs[1],state_q_idxs[2],state_q_idxs[3],:].argmax()\n",
        "\n",
        "    return action\n",
        "\n",
        "def get_q_value(Q_sa, state, action):\n",
        "    '''\n",
        "    used to get Q value for any given state and action, given the Q table\n",
        "\n",
        "    '''\n",
        "    state_q_idxs = get_state_q_indices(state)\n",
        "    q_value = Q_sa[state_q_idxs[0],state_q_idxs[1],state_q_idxs[2],state_q_idxs[3],action]\n",
        "\n",
        "    return q_value\n",
        "#print(current_sum_to_index2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILXd2gPKmo9r"
      },
      "outputs": [],
      "source": [
        "Q = np.zeros((28, 25, 2, 2, 2))\n",
        "#Q = np.full((28, 25, 2, 2, 2), -1.1)\n",
        "\n",
        "V_Q_dict = {}\n",
        "# Update V_Q_dict using correct ranges\n",
        "for player_sum in range(4, 31):   # Player's sum\n",
        "  for dealer_sum in range(4, 29):  # Dealer's sum\n",
        "    for usable_ace in [False, True]:  # Boolean usable ace\n",
        "      for stick_happened in [False, True]:  # Boolean stick_happened\n",
        "          state = (player_sum, dealer_sum, usable_ace, stick_happened)\n",
        "          # Apply the filtering conditions\n",
        "          if not (state[0] < 12 and state[2]) and not (state[0] >= 21 and not state[3]) and not (state[1] >= 21 and not state[3]):\n",
        "              V_Q_dict[state] = 0\n",
        "#print(V_Q_dict)\n",
        "\n",
        "episode_count = 0\n",
        "total_episodes = 5000\n",
        "gamma = 1             #### the discount factor\n",
        "alpha = 0.05             #### learning rate\n",
        "theta=1e-15\n",
        "bj = BlackJackEnv()\n",
        "\n",
        "# Initialize variables for tracking runtime and errors\n",
        "start_time = time.time()\n",
        "diffs_ql = []\n",
        "\n",
        "\n",
        "while episode_count < total_episodes:\n",
        "\n",
        "\n",
        "    bj.reset()  ### Initialize S (the environment's starting state)\n",
        "\n",
        "\n",
        "    current_state = bj.get_current_state()\n",
        "    current_action = get_max_action(Q, current_state)\n",
        "\n",
        "\n",
        "    ### Take Action\n",
        "    step_result = bj.step(current_action)\n",
        "\n",
        "    next_state = bj.get_current_state()\n",
        "    next_max_action = get_max_action(Q, next_state)\n",
        "    immediate_reward = step_result['reward']\n",
        "\n",
        "    next_state_q_idxs = get_state_q_indices(next_state)\n",
        "\n",
        "    #### Get Q value for the next state and max action in the next state\n",
        "    q_max_s_a = get_q_value(Q, next_state, next_max_action)\n",
        "    #print(immediate_reward)\n",
        "    td_target = immediate_reward + gamma * q_max_s_a\n",
        "\n",
        "    #### Getting Q value for the current state and action\n",
        "    q_current_s_a = get_q_value(Q, current_state, current_action)\n",
        "\n",
        "    td_error = td_target - q_current_s_a\n",
        "\n",
        "    state_q_idxs = get_state_q_indices(current_state)\n",
        "\n",
        "    #### Updating current Q(S,A)\n",
        "    Q[state_q_idxs[0],state_q_idxs[1],state_q_idxs[2],state_q_idxs[3],current_action] = q_current_s_a + alpha*td_error\n",
        "    #V_Q_dict[(state_q_idxs[0]+4,state_q_idxs[1]+4,state_q_idxs[2],state_q_idxs[3])] = Q[state_q_idxs[0], state_q_idxs[1], state_q_idxs[2], state_q_idxs[3], :].min()\n",
        "\n",
        "    current_state = next_state  ### S=S'\n",
        "\n",
        "    alpha = 1.0 / (1 + Q[state_q_idxs[0], state_q_idxs[1], state_q_idxs[2], state_q_idxs[3], current_action])\n",
        "\n",
        "    if step_result['is_done']:\n",
        "        episode_count+=1\n",
        "        #print(current_state)\n",
        "\n",
        "        for (i, j, k, l), _ in V_Q_dict.items():\n",
        "            #print((i, j, k, l))\n",
        "            k = int(k)\n",
        "            l = int(l)\n",
        "            V_Q_dict[(i, j, k, l)] = Q[i-4, j-4, k, l].min()\n",
        "\n",
        "        # Compute sup norm\n",
        "        sup_norm = max(abs(V_opt[key] - V_Q_dict[key]) for key in V_opt)\n",
        "\n",
        "        diffs_ql.append(sup_norm)\n",
        "\n",
        "        if sup_norm < theta:\n",
        "            break\n",
        "\n",
        "        if episode_count%10000 == 0:\n",
        "            print('---------Episode - {} -----------'.format(episode_count))\n",
        "\n",
        "# Calculate total runtime\n",
        "end_time = time.time()\n",
        "runtime = end_time - start_time\n",
        "\n",
        "print(f\"Total runtime: {runtime} seconds\")\n",
        "\n",
        "if diffs_ql and diffs_ql[-1] < 0:\n",
        "    diffs_ql[-1] = 0\n",
        "\n",
        "print(len(diffs_ql))\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(diffs_ql)\n",
        "#plt.plot(range(1, len(deltas)+1), deltas)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Delta')\n",
        "plt.title('Convergence of Q Learning'),\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.savefig(\"q_learning.jpg\", format=\"jpg\", dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5affT_wCnaGQ"
      },
      "outputs": [],
      "source": [
        "def get_min_action(Q_sa, current_state):\n",
        "\n",
        "    '''\n",
        "    used to get the action with the max q-value given the current state and the Q table\n",
        "\n",
        "    '''\n",
        "\n",
        "    state_q_idxs = get_state_q_indices(current_state)\n",
        "    action = Q_sa[state_q_idxs[0],state_q_idxs[1],state_q_idxs[2],state_q_idxs[3],:].argmin()\n",
        "\n",
        "    return action\n",
        "\n",
        "Q = np.zeros((28, 25, 2, 2, 2))\n",
        "#Q = np.full((28, 25, 2, 2, 2), -1.1)\n",
        "\n",
        "V_Q_dict = {}\n",
        "# Update V_Q_dict using correct ranges\n",
        "for player_sum in range(4, 31):   # Player's sum\n",
        "  for dealer_sum in range(4, 29):  # Dealer's sum\n",
        "    for usable_ace in [False, True]:  # Boolean usable ace\n",
        "      for stick_happened in [False, True]:  # Boolean stick_happened\n",
        "          state = (player_sum, dealer_sum, usable_ace, stick_happened)\n",
        "          # Apply the filtering conditions\n",
        "          if not (state[0] < 12 and state[2]) and not (state[0] >= 21 and not state[3]) and not (state[1] >= 21 and not state[3]):\n",
        "              V_Q_dict[state] = 0\n",
        "#print(V_Q_dict)\n",
        "\n",
        "episode_count = 0\n",
        "total_episodes = 5000\n",
        "gamma = 1             #### the discount factor\n",
        "alpha = 0.1             #### learning rate\n",
        "theta=1e-15\n",
        "bj = BlackJackEnv()\n",
        "\n",
        "# Initialize variables for tracking runtime and errors\n",
        "start_time = time.time()\n",
        "diffs_ql = []\n",
        "\n",
        "\n",
        "while episode_count < total_episodes:\n",
        "\n",
        "\n",
        "    bj.reset()  ### Initialize S (the environment's starting state)\n",
        "\n",
        "\n",
        "    current_state = bj.get_current_state()\n",
        "    current_action = get_min_action(Q, current_state)\n",
        "\n",
        "\n",
        "    ### Take Action\n",
        "    step_result = bj.step(current_action)\n",
        "\n",
        "    next_state = bj.get_current_state()\n",
        "    next_max_action = get_min_action(Q, next_state)\n",
        "    immediate_reward = step_result['reward']\n",
        "\n",
        "    next_state_q_idxs = get_state_q_indices(next_state)\n",
        "\n",
        "    #### Getting Q value for the current state and action\n",
        "    q_current_s_a = get_q_value(Q, current_state, current_action)\n",
        "\n",
        "    #### Get Q value for the next state and max action in the next state\n",
        "    q_max_s_a = get_q_value(Q, next_state, next_max_action)\n",
        "\n",
        "    state_q_idxs = get_state_q_indices(current_state)\n",
        "\n",
        "    #### Updating current Q(S,A)\n",
        "    Q[state_q_idxs[0],state_q_idxs[1],state_q_idxs[2],state_q_idxs[3],current_action] = (1-alpha)*q_current_s_a + alpha*(immediate_reward + q_max_s_a)\n",
        "\n",
        "    current_state = next_state  ### S=S'\n",
        "\n",
        "    alpha = 1.0 / (1 + Q[state_q_idxs[0], state_q_idxs[1], state_q_idxs[2], state_q_idxs[3], current_action])\n",
        "\n",
        "    if step_result['is_done']:\n",
        "        episode_count+=1\n",
        "        #print(current_state)\n",
        "\n",
        "        for (i, j, k, l), _ in V_Q_dict.items():\n",
        "            #print((i, j, k, l))\n",
        "            k = int(k)\n",
        "            l = int(l)\n",
        "            V_Q_dict[(i, j, k, l)] = Q[i-4, j-4, k, l].min()\n",
        "\n",
        "        # Compute sup norm\n",
        "        sup_norm = max(abs(V_opt[key] - V_Q_dict[key]) for key in V_opt)\n",
        "\n",
        "        diffs_ql.append(sup_norm)\n",
        "\n",
        "        if sup_norm < theta:\n",
        "            break\n",
        "\n",
        "        if episode_count%10000 == 0:\n",
        "            print('---------Episode - {} -----------'.format(episode_count))\n",
        "\n",
        "# Calculate total runtime\n",
        "end_time = time.time()\n",
        "runtime = end_time - start_time\n",
        "\n",
        "if diffs_ql and diffs_ql[-1] < 0:\n",
        "    diffs_ql[-1] = 0\n",
        "print(len(diffs_ql))\n",
        "print(f\"Total runtime: {runtime} seconds\")\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(diffs_ql)\n",
        "#plt.plot(range(1, len(deltas)+1), deltas)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Delta')\n",
        "plt.title('Convergence of Q Learning'),\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.savefig(\"q_learning.jpg\", format=\"jpg\", dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42fk3K1OoIrP"
      },
      "outputs": [],
      "source": [
        "zero_value_states = [state for state, value in V_Q_dict.items() if value == 0]\n",
        "for i in zero_value_states:\n",
        "  print(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_2pPeIw_-Jt"
      },
      "source": [
        "####SARSA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tRzWfz9hHur"
      },
      "outputs": [],
      "source": [
        "def get_action_epsilon_greedy(Q_sa, current_state, epsilon):\n",
        "    '''\n",
        "    Get action using epsilon-greedy policy.\n",
        "    '''\n",
        "    random_number = np.random.rand()\n",
        "    #print(random_number)\n",
        "    if random_number < epsilon:\n",
        "        return np.random.choice([0, 1])  # random action\n",
        "    else:\n",
        "        return get_max_action(Q_sa, current_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yu6bvkiUhHus"
      },
      "outputs": [],
      "source": [
        "Q = np.zeros((28, 25, 2, 2, 2))\n",
        "#Q = np.full((28, 25, 2, 2, 2), -1.1)\n",
        "\n",
        "V_Q_dict = {}\n",
        "# Update V_Q_dict using correct ranges\n",
        "for player_sum in range(4, 31):   # Player's sum\n",
        "  for dealer_sum in range(4, 29):  # Dealer's sum\n",
        "    for usable_ace in [False, True]:  # Boolean usable ace\n",
        "      for stick_happened in [False, True]:  # Boolean stick_happened\n",
        "          state = (player_sum, dealer_sum, usable_ace, stick_happened)\n",
        "          # Apply the filtering conditions\n",
        "          if not (state[0] < 12 and state[2]) and not (state[0] >= 21 and not state[3]) and not (state[1] >= 21 and not state[3]):\n",
        "              V_Q_dict[state] = 0\n",
        "#print(V_Q_dict)\n",
        "\n",
        "episode_count = 0\n",
        "total_episodes = 5000\n",
        "gamma = 1             #### the discount factor\n",
        "alpha = 0.1             #### learning rate\n",
        "epsilon = 0.1           #### epsilon for epsilon-greedy policy\n",
        "theta=1e-15\n",
        "bj = BlackJackEnv()\n",
        "\n",
        "#epsilon_min = 0.01       # Lower bound for epsilon (to maintain some exploration)\n",
        "#epsilon_decay = 0.9999\n",
        "\n",
        "# Initialize variables for tracking runtime and errors\n",
        "start_time = time.time()\n",
        "diffs_sarsa = []\n",
        "\n",
        "while episode_count < total_episodes:\n",
        "    bj.reset()  ### Initialize S (the environment's starting state)\n",
        "\n",
        "    current_state = bj.get_current_state()\n",
        "    current_action = get_action_epsilon_greedy(Q, current_state, epsilon)\n",
        "\n",
        "    step_result = bj.step(current_action)\n",
        "\n",
        "    next_state = bj.get_current_state()\n",
        "    next_action = get_action_epsilon_greedy(Q, next_state, epsilon)\n",
        "    immediate_reward = step_result['reward']\n",
        "\n",
        "\n",
        "    q_current_s_a = get_q_value(Q, current_state, current_action)\n",
        "    q_next_s_a = get_q_value(Q, next_state, next_action)\n",
        "\n",
        "    td_target = immediate_reward + gamma * q_next_s_a\n",
        "    td_error = td_target - q_current_s_a\n",
        "\n",
        "    Q_state_idxs = get_state_q_indices(current_state)\n",
        "\n",
        "    Q[Q_state_idxs[0], Q_state_idxs[1], Q_state_idxs[2], Q_state_idxs[3], current_action] = q_current_s_a + alpha * td_error\n",
        "    #V_Q_dict[(Q_state_idxs[0]+4, Q_state_idxs[1]+4, Q_state_idxs[2], Q_state_idxs[3])] = Q[Q_state_idxs[0], Q_state_idxs[1], Q_state_idxs[2], Q_state_idxs[3], :].min()\n",
        "\n",
        "    current_state = next_state  ### S=S'\n",
        "    current_action = next_action  ### A=A'\n",
        "\n",
        "    alpha = 1.0 / (1 + Q[Q_state_idxs[0], Q_state_idxs[1], Q_state_idxs[2], Q_state_idxs[3], current_action])\n",
        "    #alpha = max(0.05, 1.0 / (1 + 0.01 * episode_count))\n",
        "    #alpha = 1.0 / (1 + episode_count)\n",
        "\n",
        "\n",
        "    if step_result['is_done']:\n",
        "        episode_count+=1\n",
        "\n",
        "        for (i, j, k, l), _ in V_Q_dict.items():\n",
        "            #print((i, j, k, l))\n",
        "            k = int(k)\n",
        "            l = int(l)\n",
        "            V_Q_dict[(i, j, k, l)] = Q[i-4, j-4, k, l].min()\n",
        "\n",
        "        # Compute sup norm\n",
        "        sup_norm = max(abs(V_opt[key] - V_Q_dict[key]) for key in V_opt)\n",
        "\n",
        "        #epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
        "\n",
        "        diffs_sarsa.append(sup_norm)\n",
        "\n",
        "        if sup_norm < theta:\n",
        "            break\n",
        "\n",
        "        if episode_count%10000 == 0:\n",
        "            print('---------Episode - {} -----------'.format(episode_count))\n",
        "\n",
        "# Calculate total runtime\n",
        "end_time = time.time()\n",
        "runtime = end_time - start_time\n",
        "\n",
        "print(f\"Total runtime: {runtime} seconds\")\n",
        "\n",
        "if diffs_sarsa and diffs_sarsa[-1] < 0:\n",
        "    diffs_sarsa[-1] = 0\n",
        "\n",
        "print(len(diffs_sarsa))\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(diffs_sarsa)\n",
        "#plt.plot(range(1, len(deltas)+1), deltas)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Delta')\n",
        "plt.title('Convergence of SARSA')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.savefig(\"sarsa.jpg\", format=\"jpg\", dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p472ctRw98Nl"
      },
      "outputs": [],
      "source": [
        "Q = np.zeros((28, 25, 2, 2, 2))\n",
        "#Q = np.full((28, 25, 2, 2, 2), -1.1)\n",
        "\n",
        "V_Q_dict = {}\n",
        "# Update V_Q_dict using correct ranges\n",
        "for player_sum in range(4, 31):   # Player's sum\n",
        "  for dealer_sum in range(4, 29):  # Dealer's sum\n",
        "    for usable_ace in [False, True]:  # Boolean usable ace\n",
        "      for stick_happened in [False, True]:  # Boolean stick_happened\n",
        "          state = (player_sum, dealer_sum, usable_ace, stick_happened)\n",
        "          # Apply the filtering conditions\n",
        "          if not (state[0] < 12 and state[2]) and not (state[0] >= 21 and not state[3]) and not (state[1] >= 21 and not state[3]):\n",
        "              V_Q_dict[state] = 0\n",
        "#print(V_Q_dict)\n",
        "\n",
        "episode_count = 0\n",
        "total_episodes = 5000\n",
        "gamma = 1             #### the discount factor\n",
        "alpha = 0.05             #### learning rate\n",
        "epsilon = 0.1           #### epsilon for epsilon-greedy policy\n",
        "theta=1e-15\n",
        "bj = BlackJackEnv()\n",
        "\n",
        "#epsilon_min = 0.01       # Lower bound for epsilon (to maintain some exploration)\n",
        "#epsilon_decay = 0.9999\n",
        "\n",
        "# Initialize variables for tracking runtime and errors\n",
        "start_time = time.time()\n",
        "diffs_sarsa = []\n",
        "\n",
        "while episode_count < total_episodes:\n",
        "    bj.reset()  ### Initialize S (the environment's starting state)\n",
        "\n",
        "\n",
        "    current_state = bj.get_current_state()\n",
        "    current_action = get_min_action(Q, current_state)\n",
        "\n",
        "\n",
        "    ### Take Action\n",
        "    step_result = bj.step(current_action)\n",
        "\n",
        "    next_state = bj.get_current_state()\n",
        "    next_max_action = get_min_action(Q, next_state)\n",
        "    immediate_reward = step_result['reward']\n",
        "\n",
        "    next_state_q_idxs = get_state_q_indices(next_state)\n",
        "\n",
        "    #### Getting Q value for the current state and action\n",
        "    q_current_s_a = get_q_value(Q, current_state, current_action)\n",
        "\n",
        "    #### Get Q value for the next state and max action in the next state\n",
        "    q_max_s_a = get_q_value(Q, next_state, next_max_action)\n",
        "\n",
        "    state_q_idxs = get_state_q_indices(current_state)\n",
        "\n",
        "    #### Updating current Q(S,A)\n",
        "    Q[state_q_idxs[0],state_q_idxs[1],state_q_idxs[2],state_q_idxs[3],current_action] = (1-alpha)*q_current_s_a + alpha*(immediate_reward + q_max_s_a)\n",
        "\n",
        "    current_state = next_state  ### S=S'\n",
        "\n",
        "    alpha = 1.0 / (1 + Q[state_q_idxs[0], state_q_idxs[1], state_q_idxs[2], state_q_idxs[3], current_action])\n",
        "\n",
        "\n",
        "    if step_result['is_done']:\n",
        "        episode_count+=1\n",
        "\n",
        "        for (i, j, k, l), _ in V_Q_dict.items():\n",
        "            #print((i, j, k, l))\n",
        "            k = int(k)\n",
        "            l = int(l)\n",
        "            V_Q_dict[(i, j, k, l)] = Q[i-4, j-4, k, l].min()\n",
        "\n",
        "        # Compute sup norm\n",
        "        sup_norm = max(abs(V_opt[key] - V_Q_dict[key]) for key in V_opt)\n",
        "\n",
        "        #epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
        "\n",
        "        diffs_sarsa.append(sup_norm)\n",
        "\n",
        "        if sup_norm < theta:\n",
        "            break\n",
        "\n",
        "        if episode_count%10000 == 0:\n",
        "            print('---------Episode - {} -----------'.format(episode_count))\n",
        "\n",
        "# Calculate total runtime\n",
        "end_time = time.time()\n",
        "runtime = end_time - start_time\n",
        "\n",
        "print(f\"Total runtime: {runtime} seconds\")\n",
        "\n",
        "if diffs_sarsa and diffs_sarsa[-1] < 0:\n",
        "    diffs_sarsa[-1] = 0\n",
        "\n",
        "print(len(diffs_sarsa))\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(diffs_sarsa)\n",
        "#plt.plot(range(1, len(deltas)+1), deltas)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Delta')\n",
        "plt.title('Convergence of SARSA')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.savefig(\"sarsa.jpg\", format=\"jpg\", dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jqk56NVWvlPC"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(diffs_ql, label=\"Q-Learning\", linewidth=2)\n",
        "plt.plot(diffs_sarsa, label=\"SARSA\", linewidth=2)\n",
        "\n",
        "plt.xlabel(\"Number of Iterations\")\n",
        "plt.ylabel(\"Sup Norm Difference\")\n",
        "plt.title(\"Convergence of RL Methods\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig('RL_conv.jpg', format=\"jpg\", dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9-tmkUp1DNo"
      },
      "source": [
        "###Simulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hPc8YuP1CF5"
      },
      "outputs": [],
      "source": [
        "def get_min_action(Q_sa, current_state):\n",
        "\n",
        "    '''\n",
        "    used to get the action with the max q-value given the current state and the Q table\n",
        "\n",
        "    '''\n",
        "\n",
        "    state_q_idxs = get_state_q_indices(current_state)\n",
        "    action = Q_sa[state_q_idxs[0],state_q_idxs[1],state_q_idxs[2],state_q_idxs[3],:].argmin()\n",
        "\n",
        "    return action\n",
        "\n",
        "def q_learning():\n",
        "  Q = np.zeros((28, 25, 2, 2, 2))\n",
        "  #Q = np.full((28, 25, 2, 2, 2), -1.1)\n",
        "\n",
        "  V_Q_dict = {}\n",
        "  # Update V_Q_dict using correct ranges\n",
        "  for player_sum in range(4, 31):   # Player's sum\n",
        "    for dealer_sum in range(4, 29):  # Dealer's sum\n",
        "      for usable_ace in [False, True]:  # Boolean usable ace\n",
        "        for stick_happened in [False, True]:  # Boolean stick_happened\n",
        "            state = (player_sum, dealer_sum, usable_ace, stick_happened)\n",
        "            # Apply the filtering conditions\n",
        "            if not (state[0] < 12 and state[2]) and not (state[0] >= 21 and not state[3]) and not (state[1] >= 21 and not state[3]):\n",
        "                V_Q_dict[state] = 0\n",
        "  #print(V_Q_dict)\n",
        "\n",
        "  episode_count = 0\n",
        "  total_episodes = 5000\n",
        "  gamma = 1             #### the discount factor\n",
        "  alpha = 0.1             #### learning rate\n",
        "  theta=1e-15\n",
        "  bj = BlackJackEnv()\n",
        "\n",
        "  # Initialize variables for tracking runtime and errors\n",
        "  diffs_ql = []\n",
        "\n",
        "\n",
        "  while episode_count < total_episodes:\n",
        "\n",
        "\n",
        "      bj.reset()  ### Initialize S (the environment's starting state)\n",
        "\n",
        "\n",
        "      current_state = bj.get_current_state()\n",
        "      current_action = get_min_action(Q, current_state)\n",
        "\n",
        "\n",
        "      ### Take Action\n",
        "      step_result = bj.step(current_action)\n",
        "\n",
        "      next_state = bj.get_current_state()\n",
        "      next_max_action = get_min_action(Q, next_state)\n",
        "      immediate_reward = step_result['reward']\n",
        "\n",
        "      next_state_q_idxs = get_state_q_indices(next_state)\n",
        "\n",
        "      #### Getting Q value for the current state and action\n",
        "      q_current_s_a = get_q_value(Q, current_state, current_action)\n",
        "\n",
        "      #### Get Q value for the next state and max action in the next state\n",
        "      q_max_s_a = get_q_value(Q, next_state, next_max_action)\n",
        "\n",
        "      state_q_idxs = get_state_q_indices(current_state)\n",
        "\n",
        "      #### Updating current Q(S,A)\n",
        "      Q[state_q_idxs[0],state_q_idxs[1],state_q_idxs[2],state_q_idxs[3],current_action] = (1-alpha)*q_current_s_a + alpha*(immediate_reward + q_max_s_a)\n",
        "\n",
        "      current_state = next_state  ### S=S'\n",
        "\n",
        "      alpha = 1.0 / (1 + Q[state_q_idxs[0], state_q_idxs[1], state_q_idxs[2], state_q_idxs[3], current_action])\n",
        "\n",
        "      if step_result['is_done']:\n",
        "          episode_count+=1\n",
        "          #print(current_state)\n",
        "\n",
        "          for (i, j, k, l), _ in V_Q_dict.items():\n",
        "              #print((i, j, k, l))\n",
        "              k = int(k)\n",
        "              l = int(l)\n",
        "              V_Q_dict[(i, j, k, l)] = Q[i-4, j-4, k, l].min()\n",
        "\n",
        "          # Compute sup norm\n",
        "          sup_norm = max(abs(V_opt[key] - V_Q_dict[key]) for key in V_opt)\n",
        "\n",
        "          diffs_ql.append(sup_norm)\n",
        "\n",
        "          if sup_norm < theta:\n",
        "              break\n",
        "  if diffs_ql[-1] != 0:\n",
        "    diffs_ql[-1] = 0\n",
        "  return diffs_ql"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Dqy5TVt1CJy"
      },
      "outputs": [],
      "source": [
        "def sarsa():\n",
        "  Q = np.zeros((28, 25, 2, 2, 2))\n",
        "  #Q = np.full((28, 25, 2, 2, 2), -1.1)\n",
        "  #Q = np.random.uniform(low=-0.01, high=0.01, size=(28, 25, 2, 2, 2))\n",
        "\n",
        "  V_Q_dict = {}\n",
        "  # Update V_Q_dict using correct ranges\n",
        "  for player_sum in range(4, 31):   # Player's sum\n",
        "    for dealer_sum in range(4, 29):  # Dealer's sum\n",
        "      for usable_ace in [False, True]:  # Boolean usable ace\n",
        "        for stick_happened in [False, True]:  # Boolean stick_happened\n",
        "            state = (player_sum, dealer_sum, usable_ace, stick_happened)\n",
        "            # Apply the filtering conditions\n",
        "            if not (state[0] < 12 and state[2]) and not (state[0] >= 21 and not state[3]) and not (state[1] >= 21 and not state[3]):\n",
        "                V_Q_dict[state] = 0\n",
        "  #print(V_Q_dict)\n",
        "\n",
        "  episode_count = 0\n",
        "  total_episodes = 5000\n",
        "  gamma = 1             #### the discount factor\n",
        "  alpha = 0.1             #### learning rate\n",
        "  epsilon = 0.1           #### epsilon for epsilon-greedy policy\n",
        "  theta=1e-15\n",
        "  bj = BlackJackEnv()\n",
        "\n",
        "  #epsilon_min = 0.01       # Lower bound for epsilon (to maintain some exploration)\n",
        "  #epsilon_decay = 0.9999\n",
        "\n",
        "  # Initialize variables for tracking runtime and errors\n",
        "  start_time = time.time()\n",
        "  diffs_sarsa = []\n",
        "\n",
        "  while episode_count < total_episodes:\n",
        "      bj.reset()  ### Initialize S (the environment's starting state)\n",
        "\n",
        "      current_state = bj.get_current_state()\n",
        "      current_action = get_action_epsilon_greedy(Q, current_state, epsilon)\n",
        "\n",
        "      step_result = bj.step(current_action)\n",
        "\n",
        "      next_state = bj.get_current_state()\n",
        "      next_action = get_action_epsilon_greedy(Q, next_state, epsilon)\n",
        "      immediate_reward = step_result['reward']\n",
        "\n",
        "\n",
        "      q_current_s_a = get_q_value(Q, current_state, current_action)\n",
        "      q_next_s_a = get_q_value(Q, next_state, next_action)\n",
        "\n",
        "      td_target = immediate_reward + gamma * q_next_s_a\n",
        "      td_error = td_target - q_current_s_a\n",
        "\n",
        "      Q_state_idxs = get_state_q_indices(current_state)\n",
        "\n",
        "      Q[Q_state_idxs[0], Q_state_idxs[1], Q_state_idxs[2], Q_state_idxs[3], current_action] = q_current_s_a + alpha * td_error\n",
        "      #V_Q_dict[(Q_state_idxs[0]+4, Q_state_idxs[1]+4, Q_state_idxs[2], Q_state_idxs[3])] = Q[Q_state_idxs[0], Q_state_idxs[1], Q_state_idxs[2], Q_state_idxs[3], :].min()\n",
        "\n",
        "      current_state = next_state  ### S=S'\n",
        "      current_action = next_action  ### A=A'\n",
        "\n",
        "      alpha = 1.0 / (1 + Q[state_q_idxs[0], state_q_idxs[1], state_q_idxs[2], state_q_idxs[3], current_action])\n",
        "\n",
        "\n",
        "      if step_result['is_done']:\n",
        "          episode_count+=1\n",
        "\n",
        "          for (i, j, k, l), _ in V_Q_dict.items():\n",
        "              #print((i, j, k, l))\n",
        "              k = int(k)\n",
        "              l = int(l)\n",
        "              V_Q_dict[(i, j, k, l)] = Q[i-4, j-4, k, l].min()\n",
        "\n",
        "          # Compute sup norm\n",
        "          sup_norm = max(abs(V_opt[key] - V_Q_dict[key]) for key in V_opt)\n",
        "\n",
        "          #epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
        "\n",
        "          diffs_sarsa.append(sup_norm)\n",
        "\n",
        "          if sup_norm < theta:\n",
        "              break\n",
        "  if diffs_sarsa[-1] != 0:\n",
        "    diffs_sarsa[-1] = 0\n",
        "  return diffs_sarsa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WKRotVi1CRR"
      },
      "outputs": [],
      "source": [
        "from itertools import zip_longest\n",
        "q_l = []\n",
        "\n",
        "for i in range(1000):\n",
        "    diffs_ql = q_learning()\n",
        "    q_l.append(diffs_ql)\n",
        "\n",
        "\"\"\"for i in range(100000):\n",
        "    diffs_ql = q_learning()\n",
        "    if all(diffs_ql[i] <= diffs_ql[i-1] for i in range(1, len(diffs_ql))):\n",
        "        q_l.append(diffs_ql)\n",
        "    if len(q_l) == 1000:\n",
        "        break\"\"\"\n",
        "# Convert to a NumPy array for vectorized operations\n",
        "sim_array_ql = list(zip_longest(*q_l, fillvalue=np.nan))\n",
        "\n",
        "# Compute means and std deviations along axis=0 (column-wise)\n",
        "averages_ql = np.nanmean(sim_array_ql, axis=1).tolist()\n",
        "std_deviations_ql = np.nanstd(sim_array_ql, axis=1, ddof=1).tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKuzE4drnfSj"
      },
      "outputs": [],
      "source": [
        "from itertools import zip_longest\n",
        "q_s = []\n",
        "\n",
        "for i in range(1000):\n",
        "  #diffs_sa = sarsa()\n",
        "  diffs_sa = q_learning()\n",
        "  #print(len(diffs_sa))\n",
        "  q_s.append(diffs_sa)\n",
        "\n",
        "\"\"\"for i in range(100000):\n",
        "  diffs_sa = sarsa()\n",
        "  if all(diffs_sa[i] <= diffs_sa[i-1] for i in range(1, len(diffs_sa))): # and ((not np.isinf(diffs_sa).any()) and np.all(np.diff(diffs_sa) <= 0)):\n",
        "          q_s.append(diffs_sa)\n",
        "  if len(q_s) == 1000:\n",
        "          break\"\"\"\n",
        "\n",
        "# Convert to a NumPy array for vectorized operations\n",
        "sim_array_sa = list(zip_longest(*q_s, fillvalue=np.nan))\n",
        "\n",
        "# Compute means and std deviations along axis=0 (column-wise)\n",
        "averages_sa = np.nanmean(sim_array_sa, axis=1).tolist()\n",
        "std_deviations_sa = np.nanstd(sim_array_sa, axis=1, ddof=1).tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hp6SRAVwnh16"
      },
      "outputs": [],
      "source": [
        "# Convert to NumPy arrays\n",
        "averages_ql = np.array(averages_ql)\n",
        "std_deviations_ql = np.array(std_deviations_ql)\n",
        "averages_sa = np.array(averages_sa)\n",
        "std_deviations_sa = np.array(std_deviations_sa)\n",
        "\n",
        "def find_last_valid_index(avg_array, std_array):\n",
        "    for i in range(len(avg_array) - 1, -1, -1):\n",
        "        if avg_array[i] != 0 and std_array[i] != 0:\n",
        "            return i\n",
        "    return None\n",
        "\n",
        "# For Q-learning\n",
        "last_valid_index_ql = find_last_valid_index(averages_ql, std_deviations_ql)\n",
        "# For SARSA\n",
        "last_valid_index_sa = find_last_valid_index(averages_sa, std_deviations_sa)\n",
        "#last_valid_index_sa = find_last_valid_index(rep_averages_sa, rep_std_deviations_sa)\n",
        "\n",
        "# Create x-axis values\n",
        "steps_ql = np.arange(len(averages_ql))\n",
        "steps_sa = np.arange(len(averages_sa))\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(steps_ql[:400], averages_ql[:400], label='Q-learning Mean', color='blue')\n",
        "plt.fill_between(steps_ql[:400], averages_ql[:400] - std_deviations_ql[:400], averages_ql[:400] + std_deviations_ql[:400],\n",
        "                 color='blue', alpha=0.3, label='Q-learning ±1 Std Dev')\n",
        "plt.plot(steps_sa[:400], averages_sa[:400], label='SARSA Mean', color='green')\n",
        "plt.fill_between(steps_sa[:400], averages_sa[:400] - std_deviations_sa[:400], averages_sa[:400] + std_deviations_sa[:400],\n",
        "                 color='green', alpha=0.3, label='SARSA ±1 Std Dev') #[::-1]\n",
        "\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Sup Norm')\n",
        "plt.title('Learning Curve of Q-learning and SARSA')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"learning_curve.png\", dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dd_jxUnOecod"
      },
      "outputs": [],
      "source": [
        "#np.save('averages_ql.npy', averages_ql)\n",
        "#np.save('std_deviations_ql.npy', std_deviations_ql)\n",
        "np.save('averages_sa.npy', averages_sa)\n",
        "np.save('std_deviations_sa.npy', std_deviations_sa)\n",
        "#np.save('sim_array_ql.npy', sim_array_ql)\n",
        "np.save('sim_array_sa.npy', sim_array_sa)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDLPsjHEJGZe"
      },
      "outputs": [],
      "source": [
        "#averages_ql = np.load('averages_ql.npy')\n",
        "#std_deviations_ql = np.load('std_deviations_ql.npy')\n",
        "averages_sa = np.load('averages_sa.npy')\n",
        "std_deviations_sa = np.load('std_deviations_sa.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdI4FHfEIaey"
      },
      "outputs": [],
      "source": [
        "# Step 1: Count only non-NaN values\n",
        "lengths_sa = [np.sum(~np.isnan(arr)) for arr in sim_array_sa]\n",
        "lengths_ql = [np.sum(~np.isnan(arr)) for arr in sim_array_ql]\n",
        "\n",
        "# Step 3: Compute stats\n",
        "mean_length = np.mean(lengths_sa)\n",
        "std_length = np.std(lengths_ql)\n",
        "\n",
        "print(f\"Mean length: {mean_length}\")\n",
        "print(f\"Standard deviation: {std_length}\")\n",
        "\n",
        "print(f\"Mean length: {mean_length}\")\n",
        "print(f\"Standard deviation: {std_length}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
